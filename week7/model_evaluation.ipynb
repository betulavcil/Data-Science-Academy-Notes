{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation in Regression Models\n",
    "To build a model to accuretly predict an unknown case, we have to perform regression evaluation after building the model.\n",
    "\n",
    "* train and test on the same dataset\n",
    "* train/test split\n",
    "\n",
    "Regression Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## train and test on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/FuelConsumption.csv\")\n",
    "cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use entire dataset for training. For example, assume that we have 10 records in our dataset. We select a small portion of the dataset, such as row numbers 6 to 9. (Independent variables)\n",
    "\n",
    "The labels are called \"Actual values\" of the test set.\n",
    "\n",
    "* high \"traning accuracy\" A high training accuracy isn't necessarily a good thing. Over-fit\n",
    "\n",
    "* low \"out-of-sample accuracy\"\n",
    "\n",
    "It's important that our models have high out-of-sample accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split\n",
    "* Mutually exclusive\n",
    "* More accurate evaluation on out-of-sample accuracy. Because the testing dataset is not part of the training dataset.\n",
    "* It is more realistic for real-world problems.\n",
    "* Train/test split is highly dependent on the datasets on which the data was trained and tested.\n",
    "Another evaluation model to resolve most of these issues: K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics in Regression\n",
    "\n",
    "### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "expected = [1.0] * 11\n",
    "\n",
    "predicted = [round(1.0 - i * 0.1, 1) for i in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"real values:\", expected)\n",
    "print(\"predicted values:\", predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(expected, predicted)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(len(expected)):\n",
    "    error = abs(expected[i] - predicted[i])\n",
    "    errors.append(error)\n",
    "    print(f\"{expected[i]} - {predicted[i]} - {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.xticks(ticks=[i for i in range(len(errors))], labels=predicted)\n",
    "pyplot.plot(errors)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(expected, predicted)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(len(expected)):\n",
    "    error = (expected[i] - predicted[i])**2\n",
    "    errors.append(error)\n",
    "    print(f\"{expected[i]} - {predicted[i]} - {error:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.xticks(ticks=[i for i in range(len(errors))], labels=predicted)\n",
    "pyplot.plot(errors)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is interpretable in the same units as the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(expected, predicted))\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative Absolute Error (RAE)\n",
    "## Relative Squared Error (RSE)\n",
    "## R-squared (r2) Score\n",
    "\n",
    "The higher the r2 score, the better the model fits your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(expected, predicted)\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developer.nvidia.com/blog/a-comprehensive-overview-of-regression-evaluation-metrics/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
